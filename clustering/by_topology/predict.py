#!/usr/bin/env python3

import pandas as pd
import numpy as np
from path import Path  # pip install --user path.py
import re
import netCDF4
from collections import namedtuple
from typing import Iterable, Any, Dict


# This program can only work with data results with following properties
MSG_LEN_START = 0
MSG_LEN_END = 10000  # last message length should be 9900
MSG_LEN_STEP = 100
STEPS_COUNT = (MSG_LEN_END - MSG_LEN_START) // MSG_LEN_STEP - 1
LENGTHS = range(MSG_LEN_START, MSG_LEN_END, MSG_LEN_STEP)
assert MSG_LEN_START + (STEPS_COUNT + 1) * MSG_LEN_STEP == MSG_LEN_END


def build_extended_node_pairs(node_pairs):
    """node_pairs - dataframe that is constructed by lom2-agnostic-code.
    node_pairs contains pairs like ("n48003", "n48006") but does not contain its
    inverse ("n48006", "n48003"). This function adds these inverse pairs."""
    inversed_node_pairs = node_pairs.rename(columns={"node1": "node2", "node2": "node1"})
    return pd.concat(
        [node_pairs, inversed_node_pairs],
        ignore_index=True, verify_integrity=True
    ) \
        .drop_duplicates(subset=["node1", "node2"]) \
        .set_index(["node1", "node2"], verify_integrity=True)


TestResults = namedtuple("TestResults", ["name", "hostnames", "medians", "means", "std_dev"])


def matrix_to_table(matrix):
    """Takes DataFrame where horizontal index (columns) is the same
    as vertical index (index). Repacks it into form of table with
    3 columns: node1, node2, value."""
    table = matrix.stack().reset_index()
    table.columns = ["node1", "node2", "value"]
    return table


TopologyData = namedtuple("TopologyData", ["node_pairs", "classes", "extended_node_pairs"])


def load_topology_data() -> TopologyData:
    """Reads pickled information about topology, which should be generated by
    lom2-agnostic-code, from this file's directory, transforms it a little bit
    and returns it."""
    node_pairs = pd.read_pickle("paths_with_classes.pkl").drop("shortest_path", axis=1)
    return TopologyData(
        node_pairs,
        pd.read_pickle("classes.pkl"),
        build_extended_node_pairs(node_pairs)
    )


def read_benchmark_hostnames(path_to_file):
    lines = path_to_file.lines()
    return (re.match(r"^(n\d{5})\.", line).groups()[0] for line in lines)


def load_nc_file_as_matrices(hostnames: Iterable[Any], path: Path) -> Dict[int, pd.DataFrame]:
    """Loads netcdf file generated by network-tests2.
    Converts it to dict where keys are message lengths, values
    are matrix-dataframes with both index and columns representing
    nodes
    """
    hostnames = list(hostnames)
    with netCDF4.Dataset(path, "r") as dataset:
        step_len = dataset["step_length"][0]
        start_len = dataset["begin_mes_length"][0]
        end_len = dataset["end_mes_length"][0]
        
        assert len(hostnames) == dataset["proc_num"][0]
        assert dataset["test_type"][0] == 1  # one-to-one
        assert start_len == MSG_LEN_START
        assert end_len == MSG_LEN_END
        assert step_len == MSG_LEN_STEP

        return {
            length: pd.DataFrame(dataset["data"][i], index=hostnames, columns=hostnames)
            for (length, i) in zip(LENGTHS, range(STEPS_COUNT + 1))
        }


def load_nc_file_as_tables(hostnames: Iterable[Any], path: Path) -> Dict[int, pd.DataFrame]:
    """Loads netcdf file generated by network-tests2.
    Converts it to dict where keys are message lengths, values
    are dataframes with columns (node1, node2, value)
    """
    matrices = load_nc_file_as_matrices(hostnames, path)
    # convert matrices to tables
    return {
        length: matrix_to_table(matrix)
        for (length, matrix) in matrices.items()
    }


def load_test_results(directory: Path) -> TestResults:
    """Takes path to directory with test results generated by network-tests2 as
    argument. Returns an instance of TestResults generated from it"""
    hostnames = tuple(read_benchmark_hostnames(directory.joinpath("network_hosts.txt")))
    kwargs = {
        arg: load_nc_file_as_tables(hostnames, directory.joinpath(filename))
        for (arg, filename) in [
            ("medians", "network_median.nc"),
            ("means", "network_average.nc"),
            ("std_dev", "network_deviation.nc")
        ]
    }
    return TestResults(name=directory.basename(), hostnames=hostnames, **kwargs)


def check_all_classes_covered(topology_data: TopologyData, hostnames: Iterable[Any]) -> bool:
    """Checks if there is a class of pairs
    that was not covered by the test"""

    pairs_tested = pd.DataFrame.from_records(
        ({"node1": node1, "node2": node2}
         for node1 in hostnames for node2 in hostnames)
    )
    pairs_tested_with_class = pairs_tested \
        .join(topology_data.extended_node_pairs, on=["node1", "node2"])
    return len(pairs_tested_with_class["class_"].unique()) == len(topology_data.classes)


class Predictor():
    """Predicts ping for a packet with specific message_size between 2 nodes,
    measured in seconds."""

    def __init__(self, topology_data, test_results):
        self._classes = topology_data.classes
        self._extended_node_pairs = topology_data.extended_node_pairs

        # build tables with 2 columns each: class_, ping. There will be many rows with same class_
        pings_classes = (
            test_results.medians[msg_len]
                .join(self._extended_node_pairs, on=["node1", "node2"], how="left")
                .reset_index(drop=True)
            for msg_len in LENGTHS
        )
        
        # reverse lookup table (by message length and class)
        self._data = pd.concat(
            {
                msg_len: df.groupby("class_").mean()
                for (msg_len, df) in zip(LENGTHS, pings_classes)
            },
            names=["msg_len", "class_"]
        ).rename(columns={"value": "mean_of_medians_ping"})
                
    
    def _get_class(self, node1, node2):
        return self._extended_node_pairs.loc[node1, node2]["class_"]
    
    def predict_many(self, df: pd.DataFrame) -> pd.DataFrame:
        """df must have columns: msg_len, node1, node2.
        Returns table with rows in the same order, and column of ping predictions appended."""
        return df \
            .join(self._extended_node_pairs, on=["node1", "node2"], how="left") \
            .join(self._data, on=["msg_len", "class_"], how="left") \
            .rename(columns={"mean_of_medians_ping": "predicted_ping"}) \
            .drop(labels=["class_"], axis=1)
    
    def predict(self, msg_len, node1, node2):
        """This function takes about 1ms"""
        return self._data.loc[msg_len, self._get_class(node1, node2)].iloc[0]


def main():
    topology_data = load_topology_data()
    test_results = load_test_results(Path("/home/shibbiry/Dropbox/documents/msu/clust_top/test_results/2017-02-12__118_nodes/"))
    assert check_all_classes_covered(topology_data, test_results.hostnames)
    predictor = Predictor(topology_data, test_results)
    assert predictor.predict(4000, "n48003", "n48009") == 5.0074094301694399e-06

    def get_random_samples(count):
        return topology_data.node_pairs.sample(n=count)[["node1", "node2"]] \
            .assign(msg_len=np.random.randint(0, STEPS_COUNT, size=count) * MSG_LEN_STEP)

    print(predictor.predict_many(get_random_samples(10)))


if __name__ == "__main__":
    main()
